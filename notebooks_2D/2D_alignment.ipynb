{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import tensorflow as tf\n",
    "import tf_slim as slim\n",
    "from graphgan.utils import get_3d_direction\n",
    "from graphgan.layers import *\n",
    "from graphgan.gradient_penalty import gradient_penaly\n",
    "from graphgan.datasets import graph_input_fn\n",
    "from graphgan.datasets import project_ellipticities, project_3d_shape, project_ellipticities_np\n",
    "from astropy.table import Table, join\n",
    "from functools import partial\n",
    "from halotools_ia.correlation_functions  import ed_3d,ee_3d, ed_3d_one_two_halo_decomp\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import tensorflow_probability as tfp\n",
    "import pickle\n",
    "from pandas import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Available: \",  (tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the catalog and do some pre-processing,\n",
    "\n",
    "tng = pickle.load(  open('/hildafs/projects/phy200017p/yjagvara/some_data/TNG100-1_99_non-reduced_galaxy_shapes_multi_scale_1024_MLP_only_cent.pkl', \"rb\" ) )\n",
    "\n",
    "tng = tng[tng['dm_mass']>0]\n",
    "tng = tng[[log10(tng['dm_mass']*10**10)>9]]\n",
    "tng = tng[[log10(tng['mass']*10**10)>9]]\n",
    "tng['q'] = tng['b']/tng['a']\n",
    "tng['s'] = tng['c']/tng['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_shape(a3d, b3d, c3d, q3d, s3d):\n",
    " \n",
    "    \n",
    "    s = tf.stack([a3d, b3d, c3d])\n",
    "     \n",
    "    w = tf.stack([tf.ones_like(q3d), q3d, s3d])\n",
    " \n",
    "\n",
    "    k = tf.reduce_sum(s[:,:,0:2]*tf.expand_dims(s[:,:,2], axis=-2) / tf.expand_dims(w[:,:]**2, axis=-2), axis=0)\n",
    "    a2 =tf.reduce_sum(s[:,:,2]**2/w[:,:]**2, axis=0)\n",
    " \n",
    "    first_term = tf.reduce_sum(tf.einsum('ijko,ijlo->ijklo', s[:,:,0:2,...], s[:,:,0:2,...]) / tf.expand_dims(tf.expand_dims(w[:,:]**2,-2),-2), axis=0)\n",
    "    second_term = tf.einsum('ijo,iko->ijko', k,k)/tf.expand_dims(tf.expand_dims(a2,-2),-2)\n",
    "    Winv = first_term - second_term\n",
    "    \n",
    "    W = tf.linalg.inv(tf.squeeze( tf.transpose(Winv) ))\n",
    "    d = tf.sqrt(tf.linalg.det(W))\n",
    "    denom = ( W[:,0,0] + W[:,1,1] + 2*d)\n",
    "    num_1 = (W[:,0,0] - W[:,1,1])\n",
    "    \n",
    "    e1 = num_1/denom\n",
    "    e2 = 2 * W[:,0,1]/denom\n",
    " \n",
    "    return tf.stack([e1, e2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will get the 2D vectors of interest\n",
    "a3d = np.array([[tng['av_x'] , tng['av_y'] , tng['av_z'] ]])\n",
    "b3d = np.array([[tng['bv_x'] , tng['bv_y'] , tng['bv_z'] ]])\n",
    "c3d = np.array([[tng['cv_x'] , tng['cv_y'] , tng['cv_z'] ]])\n",
    "q3d = np.array([tng['b'] /tng['a'] ])\n",
    "s3d = np.array([tng['c'] /tng['a'] ])\n",
    "    \n",
    "e12_modif = project_3d_shape(a3d, b3d, c3d, q3d, s3d)\n",
    " \n",
    "tng['e1'] = e12_modif[:,0]\n",
    "tng['e2'] = e12_modif[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reoriente all galaxies with respect to the tidal field\n",
    "# Pre-processing the orientation of galaxies with respect to their host haloes and the tidal field\n",
    "\n",
    "# Computes the size of groups\n",
    "gids, idx, inv, counts  = np.unique(tng['GroupID'],  return_index=True, return_inverse=True, return_counts=True)\n",
    "tng['group_size'] = counts[inv]\n",
    "\n",
    "# Convert distances to Mpc\n",
    "# tng['gal_pos_x'] /= 1000.\n",
    "# tng['gal_pos_y'] /= 1000.\n",
    "# tng['gal_pos_z'] /= 1000.\n",
    "\n",
    "tng['group_x'] /= 1000.\n",
    "tng['group_y'] /= 1000.\n",
    "tng['group_z'] /= 1000.\n",
    "\n",
    "# Computes direction to the central\n",
    "tng['cen_x'] = tng['group_x'][idx][inv] - tng['gal_pos_x']\n",
    "tng['cen_y'] = tng['group_y'][idx][inv] - tng['gal_pos_y']\n",
    "tng['cen_z'] = tng['group_z'][idx][inv] - tng['gal_pos_z']\n",
    "ncen = np.sqrt(tng['cen_x']**2 + tng['cen_y']**2 + tng['cen_z']**2 ) \n",
    "tng['cen_r'] = ncen\n",
    "\n",
    "inds_cent = ncen == 0\n",
    "ncen[ncen == 0] = 1\n",
    "tng['cen_x'] = tng['cen_x']/ncen\n",
    "tng['cen_y'] = tng['cen_y']/ncen\n",
    "tng['cen_z'] = tng['cen_z']/ncen\n",
    "\n",
    "# # First reorienting the tidal \n",
    "a = (tng['tid_av_x_0.1_1024']*tng['cen_x'] +\n",
    "     tng['tid_av_y_0.1_1024']*tng['cen_y'] +\n",
    "     tng['tid_av_z_0.1_1024']*tng['cen_z'])\n",
    "\n",
    "# According to the sign, decide to reverse the orientation of\n",
    "flip_a = ones_like(a)\n",
    "flip_a[where(a < 0)] *= -1.0\n",
    "tng['tid_av_x_0.1_1024'] *= flip_a\n",
    "tng['tid_av_y_0.1_1024'] *= flip_a\n",
    "tng['tid_av_z_0.1_1024'] *= flip_a\n",
    "tng['tid_bv_x_0.1_1024'] *= flip_a\n",
    "tng['tid_bv_y_0.1_1024'] *= flip_a\n",
    "tng['tid_bv_z_0.1_1024'] *= flip_a\n",
    "\n",
    "# Computing angle with respect to the tidal field,\n",
    "# adjusting the axes to have the same orientation\n",
    "aTid = (tng['dm_av_x']*tng['tid_cv_x_0.1_1024'] +\n",
    "        tng['dm_av_y']*tng['tid_cv_y_0.1_1024'] +\n",
    "        tng['dm_av_z']*tng['tid_cv_z_0.1_1024'])\n",
    "bTid = (tng['dm_bv_x']*tng['tid_cv_x_0.1_1024'] +\n",
    "        tng['dm_bv_y']*tng['tid_cv_y_0.1_1024'] +\n",
    "        tng['dm_bv_z']*tng['tid_cv_z_0.1_1024'])\n",
    "cTid = (tng['dm_cv_x']*tng['tid_cv_x_0.1_1024'] +\n",
    "        tng['dm_cv_y']*tng['tid_cv_y_0.1_1024'] +\n",
    "        tng['dm_cv_z']*tng['tid_cv_z_0.1_1024'])\n",
    "caTid = (tng['dm_cv_x']*tng['tid_av_x_0.1_1024'] +\n",
    "         tng['dm_cv_y']*tng['tid_av_y_0.1_1024'] +\n",
    "         tng['dm_cv_z']*tng['tid_av_z_0.1_1024'])\n",
    "\n",
    "# According to the sign, decide to reverse the orientation of\n",
    "# the dark matter halo by rotating around b or c\n",
    "flip_a = ones_like(aTid)\n",
    "flip_a[where(aTid < 0)] *= -1.0\n",
    "flip_c = ones_like(cTid)\n",
    "flip_c[where(caTid < 0)] *= -1.0\n",
    "flip_b = ones_like(bTid)\n",
    "flip_b = flip_a * flip_c\n",
    "\n",
    "# Apply rotation around c, thus preserving the sign of c\n",
    "aTid *= flip_a\n",
    "bTid *= flip_b\n",
    "cTid *= flip_c\n",
    "\n",
    "# Update the DM halo orientation\n",
    "tng['dm_av_x'] *= flip_a\n",
    "tng['dm_av_y'] *= flip_a\n",
    "tng['dm_av_z'] *= flip_a\n",
    "tng['dm_bv_x'] *= flip_b\n",
    "tng['dm_bv_y'] *= flip_b\n",
    "tng['dm_bv_z'] *= flip_b\n",
    "tng['dm_cv_x'] *= flip_c\n",
    "tng['dm_cv_y'] *= flip_c\n",
    "tng['dm_cv_z'] *= flip_c\n",
    "\n",
    "# Compute misalignment of stellar component in same rotated frame\n",
    "a = (tng['dm_av_x']*tng['av_x'] +\n",
    "     tng['dm_av_y']*tng['av_y'] +\n",
    "     tng['dm_av_z']*tng['av_z'])\n",
    "b = (tng['dm_bv_x']*tng['av_x'] +\n",
    "     tng['dm_bv_y']*tng['av_y'] +\n",
    "     tng['dm_bv_z']*tng['av_z'])\n",
    "c = (tng['dm_cv_x']*tng['av_x'] +\n",
    "     tng['dm_cv_y']*tng['av_y'] +\n",
    "     tng['dm_cv_z']*tng['av_z'])\n",
    "cc = (tng['dm_cv_x']*tng['cv_x'] +\n",
    "      tng['dm_cv_y']*tng['cv_y'] +\n",
    "      tng['dm_cv_z']*tng['cv_z'])\n",
    "\n",
    "# Apply rotation to the Stellar shape frame to match DM frame\n",
    "flip_a_stel = ones_like(a)\n",
    "flip_a_stel[where(a < 0)] *= -1.0\n",
    "flip_c_stel = ones_like(cc)\n",
    "flip_c_stel[where(cc < 0)] *= -1.0\n",
    "flip_b_stel = flip_a_stel * flip_c_stel\n",
    "\n",
    "# Rotation around c, leaving c unchanged\n",
    "# Rotation around a, leaving a unchanged\n",
    "a *= flip_a_stel\n",
    "b *= flip_b_stel\n",
    "c *= flip_c_stel\n",
    "\n",
    "# Update the Stellar halo orientation\n",
    "tng['av_x'] *= flip_a_stel\n",
    "tng['av_y'] *= flip_a_stel\n",
    "tng['av_z'] *= flip_a_stel\n",
    "tng['bv_x'] *= flip_b_stel\n",
    "tng['bv_y'] *= flip_b_stel\n",
    "tng['bv_z'] *= flip_b_stel\n",
    "tng['cv_x'] *= flip_c_stel\n",
    "tng['cv_y'] *= flip_c_stel\n",
    "tng['cv_z'] *= flip_c_stel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing some preprocessing\n",
    "tng['tot_mass'] = log10(tng['tot_mass']*1e10)\n",
    "tng['group_mass'] = log10(tng['group_mass']*1e10)\n",
    "tng['mass'] = log10(tng['mass']*1e10)#*1e10+1)\n",
    "tng['dm_mass'] = log10(tng['dm_mass']*1e10)#*1e10+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = tng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog['dm_mass_scaled'] = clip(RobustScaler().fit_transform(catalog['dm_mass'].reshape((-1,1))),-5,5).squeeze()\n",
    "catalog['mass_scaled'] = clip(RobustScaler().fit_transform(catalog['mass'].reshape((-1,1))),-5,5).squeeze()\n",
    "catalog['group_mass_scaled'] = clip(RobustScaler().fit_transform(catalog['group_mass'].reshape((-1,1))),-5,5).squeeze()\n",
    "catalog['tid_a_0.1_1024_scaled'] = clip(RobustScaler().fit_transform(catalog['tid_a_0.1_1024'].reshape((-1,1))),-5,5).squeeze()\n",
    "catalog['tid_b_0.1_1024_scaled'] = clip(RobustScaler().fit_transform(catalog['tid_b_0.1_1024'].reshape((-1,1))),-5,5).squeeze()\n",
    "catalog['tid_c_0.1_1024_scaled'] = clip(RobustScaler().fit_transform(catalog['tid_c_0.1_1024'].reshape((-1,1))),-5,5).squeeze()\n",
    "catalog['tid_a_0.5_1024_scaled'] = clip(RobustScaler().fit_transform(catalog['tid_a_0.5_1024'].reshape((-1,1))),-5,5).squeeze()\n",
    "catalog['tid_b_0.5_1024_scaled'] = clip(RobustScaler().fit_transform(catalog['tid_b_0.5_1024'].reshape((-1,1))),-5,5).squeeze()\n",
    "catalog['tid_c_0.5_1024_scaled'] = clip(RobustScaler().fit_transform(catalog['tid_c_0.5_1024'].reshape((-1,1))),-5,5).squeeze()\n",
    "catalog['tid_a_1.0_1024_scaled'] = clip(RobustScaler().fit_transform(catalog['tid_a_1.0_1024'].reshape((-1,1))),-5,5).squeeze()\n",
    "catalog['tid_b_1.0_1024_scaled'] = clip(RobustScaler().fit_transform(catalog['tid_b_1.0_1024'].reshape((-1,1))),-5,5).squeeze()\n",
    "catalog['tid_c_1.0_1024_scaled'] = clip(RobustScaler().fit_transform(catalog['tid_c_1.0_1024'].reshape((-1,1))),-5,5).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighting='exp'\n",
    "directions = get_3d_direction()\n",
    "filter_size=directions.shape[-1]\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "# Define generator function\n",
    "def conditional_generator_fn(inputs, \n",
    "                             is_training=True,\n",
    "                             reuse=None,\n",
    "                             scope='Generator',\n",
    "                             fused_batch_norm=False):\n",
    "\n",
    "    W0, W1, W2, pm0, pm1, pm2, xsp, X, noise = inputs\n",
    "    # W_i matrices define the sprase matrix to construct adjecency\n",
    "    adj = tf.SparseTensor(tf.cast(W0, tf.int64),W1,W2)\n",
    "    # Computes 3D adjacency matrix\n",
    "    mr = spatial_adjacency(xsp, adj, directions, filter_size, radial_weighting=weighting, radial_scale=0.4,\n",
    "                           learn_scale=False)\n",
    "\n",
    "    # Input level, transforming all inputs features into a single channel\n",
    "    # First we mix the inputs\n",
    "    net = graph_conv2(tf.concat([noise, X],axis=1), mr, num_outputs=128, activation_fn=tf.nn.leaky_relu)\n",
    "    net = slim.batch_norm(net,center=True, scale=True)\n",
    "\n",
    "    net = graph_conv2(net, mr, num_outputs=128,  activation_fn=tf.nn.leaky_relu)\n",
    "    net = slim.batch_norm(net,center=True, scale=True)\n",
    "\n",
    "    net = graph_conv2(net, mr, num_outputs=16,  activation_fn=tf.nn.leaky_relu )\n",
    "    net = slim.batch_norm(net,center=True, scale=True )\n",
    "    \n",
    " \n",
    "    net_a2d = graph_conv2(net, mr, 2, activation_fn=tf.nn.tanh, one_hop=None, weights_initializer=tf.compat.v1.truncated_normal_initializer(mean= 0.0, stddev=0.001))\n",
    "\n",
    "    # Assemble output, \n",
    "    \n",
    "    out_net = tf.concat([net_a2d], axis=1)\n",
    "\n",
    "\n",
    "    return out_net\n",
    "\n",
    "def conditional_discriminator_fn(y, conditioning):\n",
    "    \"\"\"\n",
    "    Discriminator network that can tell if the galaxies are correctly aligned\n",
    "    args:\n",
    "    y: alignment signal\n",
    "    conditioning: tuple (adj, idn, x_spatial, pool, x, noise)\n",
    "    \"\"\"\n",
    "    W0, W1, W2, pm0, pm1, pm2, xsp, X, noise = conditioning\n",
    "    #pm_i are the pooling matrices, also sparse\n",
    "    \n",
    "    adj = tf.SparseTensor(tf.cast(W0, tf.int64),W1,W2)\n",
    "    pool = tf.SparseTensor(tf.cast(pm0, tf.int64), pm1, pm2)\n",
    "    \n",
    "    # Computes 3D adjacency matrices for each multi-resolution level\n",
    "    mr = spatial_adjacency(xsp, adj, directions, filter_size, radial_weighting=weighting, learn_scale=False, \n",
    "                           radial_scale=0.2)\n",
    "\n",
    "    if y.get_shape().as_list()[1] > 2: \n",
    "\n",
    "        y = project_3d_shape(tf.expand_dims(tf.transpose(y[...,0:3]),axis=0) , tf.expand_dims(tf.transpose(y[...,3:6]),axis=0), \n",
    "                        tf.expand_dims(tf.transpose(y[...,6:9]),axis=0), tf.expand_dims(tf.transpose(y[...,9]),axis=0), tf.expand_dims(tf.transpose(y[...,10]),axis=0))\n",
    "        \n",
    "\n",
    "    net = graph_conv2(tf.concat([y, X],axis=1), mr, num_outputs=128, activation_fn=tf.nn.leaky_relu)\n",
    "    net = graph_conv2(net, mr, 128, activation_fn=tf.nn.leaky_relu)\n",
    "    net = graph_conv2(net, mr, 64, activation_fn=tf.nn.leaky_relu)\n",
    "    net = graph_conv2(net, mr, 32, activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "    # Apply spatial pooling, MeanPooling\n",
    "    net = tf.compat.v1.sparse_tensor_dense_matmul(pool, net)\n",
    "\n",
    "    net = slim.fully_connected(net, 1, activation_fn=None)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_gan as tfgan\n",
    "from tensorflow_gan.python import namedtuples\n",
    "\n",
    "my_config = tf.estimator.RunConfig(\n",
    "    save_summary_steps = 500,\n",
    "    save_checkpoints_steps = 500,\n",
    "    keep_checkpoint_max = 50000,       # Retain the 10 most recent checkpoints.\n",
    ")\n",
    "\n",
    "def silly_custom_discriminator_loss(gan_model,reduction='', add_summaries=True):\n",
    "    return tf.reduce_mean(gan_model.discriminator_real_outputs**2)\n",
    "#\n",
    "# Initialize GANEstimator with options and hyperparameters.\n",
    "gan_estimator = tfgan.estimator.GANEstimator(\n",
    "    generator_fn=conditional_generator_fn,\n",
    "    discriminator_fn=conditional_discriminator_fn,\n",
    "    generator_loss_fn=tfgan.losses.wasserstein_generator_loss,\n",
    "    discriminator_loss_fn=lambda *args, **kwargs: (tfgan.losses.wasserstein_discriminator_loss(*args, **kwargs) + \n",
    "                                                   15*gradient_penaly(*args, **kwargs)+\n",
    "                                                   0.001*silly_custom_discriminator_loss(*args, **kwargs)),   \n",
    "    generator_optimizer=tf.compat.v1.train.AdamOptimizer(0.001, beta1=0.0, beta2=0.95),\n",
    "    discriminator_optimizer=tf.compat.v1.train.AdamOptimizer(0.001, beta1=0.0, beta2=0.95),\n",
    "    get_hooks_fn=tfgan.get_sequential_train_hooks(train_steps=namedtuples.GANTrainSteps(5, 1)),\n",
    "    model_dir='./test3_500steps', \n",
    "    config=my_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_fn = partial(graph_input_fn, catalog,\n",
    "                      scalar_features=('mass_scaled','central_bool', 'group_mass_scaled','dm_mass_scaled', \n",
    "                                       'tid_a_0.1_1024_scaled', 'tid_b_0.1_1024_scaled' ,'tid_c_0.1_1024_scaled' ,\n",
    "                                       'tid_a_0.5_1024_scaled' ,'tid_b_0.5_1024_scaled' ,'tid_c_0.5_1024_scaled' ,\n",
    "                                       'tid_a_1.0_1024_scaled' ,'tid_b_1.0_1024_scaled' ,'tid_c_1.0_1024_scaled' ), \n",
    "                      vector_features=('mlp_av_x','mlp_av_y','mlp_av_z', 'tid_av_x_0.1_1024', 'tid_av_y_0.1_1024', 'tid_av_z_0.1_1024',\n",
    "                                       'tid_bv_x_0.1_1024', 'tid_bv_y_0.1_1024', 'tid_bv_z_0.1_1024',\n",
    "                                       'tid_cv_x_0.1_1024', 'tid_cv_y_0.1_1024', 'tid_cv_z_0.1_1024',\n",
    "                                         'tid_av_x_0.5_1024', 'tid_av_y_0.5_1024', 'tid_av_z_0.5_1024',\n",
    "                                       'tid_bv_x_0.5_1024', 'tid_bv_y_0.5_1024', 'tid_bv_z_0.5_1024',\n",
    "                                       'tid_cv_x_0.5_1024', 'tid_cv_y_0.5_1024', 'tid_cv_z_0.5_1024',\n",
    "                                       'tid_av_x_1.0_1024', 'tid_av_y_1.0_1024', 'tid_av_z_1.0_1024',\n",
    "                                       'tid_bv_x_1.0_1024', 'tid_bv_y_1.0_1024', 'tid_bv_z_1.0_1024',\n",
    "                                       'tid_cv_x_1.0_1024', 'tid_cv_y_1.0_1024', 'tid_cv_z_1.0_1024'),\n",
    "                      \n",
    "                      scalar_labels=('q','s'),\n",
    "                      vector_labels=('av_x', 'av_y', 'av_z',\n",
    "                                    'bv_x', 'bv_y', 'bv_z',\n",
    "                                    'cv_x', 'cv_y', 'cv_z'),\n",
    "                      shuffle=True, rotate=True, repeat=True, noise_size=32, batch_size=64)\n",
    "\n",
    "testing_fn = partial(graph_input_fn, catalog,\n",
    "                      scalar_features=('mass_scaled','central_bool', 'group_mass_scaled','dm_mass_scaled',\n",
    "                                      'tid_a_0.1_1024_scaled', 'tid_b_0.1_1024_scaled' ,'tid_c_0.1_1024_scaled' ,\n",
    "                                       'tid_a_0.5_1024_scaled' ,'tid_b_0.5_1024_scaled' ,'tid_c_0.5_1024_scaled' ,\n",
    "                                       'tid_a_1.0_1024_scaled' ,'tid_b_1.0_1024_scaled' ,'tid_c_1.0_1024_scaled' ),#,'tid_a_scaled','tid_b_scaled','tid_c_scaled'),\n",
    "                      vector_features=('mlp_av_x','mlp_av_y','mlp_av_z','tid_av_x_0.1_1024', 'tid_av_y_0.1_1024', 'tid_av_z_0.1_1024',\n",
    "                                       'tid_bv_x_0.1_1024', 'tid_bv_y_0.1_1024', 'tid_bv_z_0.1_1024',\n",
    "                                       'tid_cv_x_0.1_1024', 'tid_cv_y_0.1_1024', 'tid_cv_z_0.1_1024',\n",
    "                                         'tid_av_x_0.5_1024', 'tid_av_y_0.5_1024', 'tid_av_z_0.5_1024',\n",
    "                                       'tid_bv_x_0.5_1024', 'tid_bv_y_0.5_1024', 'tid_bv_z_0.5_1024',\n",
    "                                       'tid_cv_x_0.5_1024', 'tid_cv_y_0.5_1024', 'tid_cv_z_0.5_1024',\n",
    "                                       'tid_av_x_1.0_1024', 'tid_av_y_1.0_1024', 'tid_av_z_1.0_1024',\n",
    "                                       'tid_bv_x_1.0_1024', 'tid_bv_y_1.0_1024', 'tid_bv_z_1.0_1024',\n",
    "                                       'tid_cv_x_1.0_1024', 'tid_cv_y_1.0_1024', 'tid_cv_z_1.0_1024' ),\n",
    "                     \n",
    "                      scalar_labels=('q','s'),\n",
    "                      vector_labels=('av_x', 'av_y', 'av_z',\n",
    "                                    'bv_x', 'bv_y', 'bv_z',\n",
    "                                    'cv_x', 'cv_y', 'cv_z'),\n",
    "                      shuffle=False, rotate=False, repeat=False, noise_size=32, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assuming everything went fine up to this point the training will start when hit ENTER\n",
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "gan_estimator.train(training_fn, steps=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
